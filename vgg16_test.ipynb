{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "vgg16_test.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JinFree/NetworkSplit/blob/main/vgg16_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "growing-weekend"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "###   http://www.gisdeveloper.co.kr/?p=8443\n",
        "SEED_NUM=42\n",
        "random.seed(a=SEED_NUM, version=2)\n",
        "np.random.seed(seed=SEED_NUM)\n",
        "torch.manual_seed(SEED_NUM)\n",
        "torch.cuda.manual_seed(SEED_NUM)\n",
        "torch.cuda.manual_seed_all(SEED_NUM)\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def input_trained_params_conv2d(src, dst, is_forward_half = True):\n",
        "    stride = 0\n",
        "    if not is_forward_half:\n",
        "        stride = int(dst.weight.shape[0])\n",
        "        \n",
        "    for i in range(dst.weight.shape[0]):\n",
        "        dst.weight[i] = src.weight[i+stride]\n",
        "        dst.bias[i] = src.bias[i+stride]\n",
        "        \n",
        "    return dst\n",
        "\n",
        "def input_trained_params_linear(src, dst):\n",
        "    dst.weight = src.weight\n",
        "    dst.bias = src.bias\n",
        "    return dst"
      ],
      "id": "growing-weekend",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "important-russia"
      },
      "source": [
        "class VGG16_HALF(nn.Module):\n",
        "    def __init__(self, num_classes = 1000, inference = False):\n",
        "        super().__init__()\n",
        "        self.inference = inference\n",
        "        self.inf_checker = 0\n",
        "        \n",
        "        self.conv_0_1 = nn.Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_0_2 = nn.Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.layer_0 = None\n",
        "        \n",
        "        self.conv_2_1 = nn.Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_2_2 = nn.Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.layer_2 = None\n",
        "        \n",
        "        self.conv_5_1 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_5_2 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.layer_5 = None\n",
        "        \n",
        "        self.conv_7_1 = nn.Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_7_2 = nn.Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.layer_7 = None\n",
        "        \n",
        "        \n",
        "        self.conv_10_1 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_10_2 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.layer_10 = None\n",
        "        \n",
        "        self.conv_12_1 = nn.Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_12_2 = nn.Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.layer_12 = None\n",
        "        \n",
        "        self.conv_14_1 = nn.Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_14_2 = nn.Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.layer_14 = None\n",
        "        \n",
        "        self.conv_17_1 = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_17_2 = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.layer_17 = None\n",
        "        \n",
        "        self.conv_19_1 = nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_19_2 = nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.layer_19 = None\n",
        "        \n",
        "        \n",
        "        self.conv_21_1 = nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_21_2 = nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.layer_21 = None\n",
        "        \n",
        "        self.conv_24_1 = nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_24_2 = nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.layer_24 = None\n",
        "        \n",
        "        self.conv_26_1 = nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_26_2 = nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.layer_26 = None\n",
        "        \n",
        "        self.conv_28_1 = nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_28_2 = nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  \n",
        "        self.layer_28 = None      \n",
        "                \n",
        "        self.result = None\n",
        "    \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "    \n",
        "    def inference_with_torchvision(self):\n",
        "        network = models.vgg16(pretrained=True)\n",
        "\n",
        "        input_trained_params_conv2d(network.features[0], self.conv_0_1, True)\n",
        "        input_trained_params_conv2d(network.features[0], self.conv_0_2, False)\n",
        "\n",
        "        input_trained_params_conv2d(network.features[2], self.conv_2_1, True)\n",
        "        input_trained_params_conv2d(network.features[2], self.conv_2_2, False)\n",
        "\n",
        "        input_trained_params_conv2d(network.features[5], self.conv_5_1, True)\n",
        "        input_trained_params_conv2d(network.features[5], self.conv_5_2, False)\n",
        "\n",
        "        input_trained_params_conv2d(network.features[7], self.conv_7_1, True)\n",
        "        input_trained_params_conv2d(network.features[7], self.conv_7_2, False)\n",
        "\n",
        "        input_trained_params_conv2d(network.features[10], self.conv_10_1, True)\n",
        "        input_trained_params_conv2d(network.features[10], self.conv_10_2, False)\n",
        "\n",
        "        input_trained_params_conv2d(network.features[12], self.conv_12_1, True)\n",
        "        input_trained_params_conv2d(network.features[12], self.conv_12_2, False)\n",
        "\n",
        "        input_trained_params_conv2d(network.features[14], self.conv_14_1, True)\n",
        "        input_trained_params_conv2d(network.features[14], self.conv_14_2, False)\n",
        "\n",
        "        input_trained_params_conv2d(network.features[17], self.conv_17_1, True)\n",
        "        input_trained_params_conv2d(network.features[17], self.conv_17_2, False)\n",
        "\n",
        "        input_trained_params_conv2d(network.features[19], self.conv_19_1, True)\n",
        "        input_trained_params_conv2d(network.features[19], self.conv_19_2, False)\n",
        "\n",
        "        input_trained_params_conv2d(network.features[21], self.conv_21_1, True)\n",
        "        input_trained_params_conv2d(network.features[21], self.conv_21_2, False)\n",
        "\n",
        "        input_trained_params_conv2d(network.features[24], self.conv_24_1, True)\n",
        "        input_trained_params_conv2d(network.features[24], self.conv_24_2, False)\n",
        "\n",
        "        input_trained_params_conv2d(network.features[26], self.conv_26_1, True)\n",
        "        input_trained_params_conv2d(network.features[26], self.conv_26_2, False)\n",
        "\n",
        "        input_trained_params_conv2d(network.features[28], self.conv_28_1, True)\n",
        "        input_trained_params_conv2d(network.features[28], self.conv_28_2, False)\n",
        "\n",
        "        input_trained_params_linear(network.classifier[0], self.classifier[0])\n",
        "        input_trained_params_linear(network.classifier[3], self.classifier[3])\n",
        "        input_trained_params_linear(network.classifier[6], self.classifier[6])\n",
        "        \n",
        "    \n",
        "    def change_status(self):\n",
        "        if self.inference:\n",
        "            self.inference = False\n",
        "        else:\n",
        "            self.inference = True\n",
        "        print(self.inference)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.inference:\n",
        "            if self.inf_checker == 0:\n",
        "                self.inf_checker += 1\n",
        "                \n",
        "                self.layer_0 = self.conv_0_1(x)\n",
        "                layer_0_2 = self.conv_0_2(x)\n",
        "                x = torch.cat((self.layer_0, layer_0_2), dim=1)\n",
        "\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "                self.layer_2 = self.conv_2_1(x)\n",
        "                layer_2_2 = self.conv_2_2(x)\n",
        "                x = torch.cat((self.layer_2, layer_2_2), dim=1)\n",
        "\n",
        "                x = F.relu(x, inplace=True)\n",
        "                x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "\n",
        "                self.layer_5 = self.conv_5_1(x)\n",
        "                layer_5_2 = self.conv_5_2(x)\n",
        "                x = torch.cat((self.layer_5, layer_5_2), dim=1)\n",
        "\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                self.layer_7 = self.conv_7_1(x)\n",
        "                layer_7_2 = self.conv_7_2(x)\n",
        "                x = torch.cat((self.layer_7, layer_7_2), dim=1)\n",
        "\n",
        "                x = F.relu(x, inplace=True)\n",
        "                x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "\n",
        "                self.layer_10 = self.conv_10_1(x)\n",
        "                layer_10_2 = self.conv_10_2(x)\n",
        "                x = torch.cat((self.layer_10, layer_10_2), dim=1)\n",
        "\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                self.layer_12 = self.conv_12_1(x)\n",
        "                layer_12_2 = self.conv_12_2(x)\n",
        "                x = torch.cat((self.layer_12, layer_12_2), dim=1)\n",
        "\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                self.layer_14 = self.conv_14_1(x)\n",
        "                layer_14_2 = self.conv_14_2(x)\n",
        "                x = torch.cat((self.layer_14, layer_14_2), dim=1)\n",
        "\n",
        "                x = F.relu(x, inplace=True)\n",
        "                x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "\n",
        "                self.layer_17 = self.conv_17_1(x)\n",
        "                layer_17_2 = self.conv_17_2(x)\n",
        "                x = torch.cat((self.layer_17, layer_17_2), dim=1)\n",
        "\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                self.layer_19 = self.conv_19_1(x)\n",
        "                layer_19_2 = self.conv_19_2(x)\n",
        "                x = torch.cat((self.layer_19, layer_19_2), dim=1)\n",
        "\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                self.layer_21 = self.conv_21_1(x)\n",
        "                layer_21_2 = self.conv_21_2(x)\n",
        "                x = torch.cat((self.layer_21, layer_21_2), dim=1)\n",
        "\n",
        "                x = F.relu(x, inplace=True)\n",
        "                x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "\n",
        "                self.layer_24 = self.conv_24_1(x)\n",
        "                layer_24_2 = self.conv_24_2(x)\n",
        "                x = torch.cat((self.layer_24, layer_24_2), dim=1)\n",
        "\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                self.layer_26 = self.conv_26_1(x)\n",
        "                layer_26_2 = self.conv_26_2(x)\n",
        "                x = torch.cat((self.layer_26, layer_26_2), dim=1)\n",
        "\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                self.layer_28 = self.conv_28_1(x)\n",
        "                layer_28_2 = self.conv_28_2(x)\n",
        "                x = torch.cat((self.layer_28, layer_28_2), dim=1)\n",
        "\n",
        "                x = F.relu(x, inplace=True)\n",
        "                self.result = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "                \n",
        "            elif self.inf_checker == 1:\n",
        "                self.inf_checker += 1\n",
        "                \n",
        "                layer_0_2 = self.conv_0_2(x)\n",
        "                x = torch.cat((self.layer_0, layer_0_2), dim=1)\n",
        "                self.layer_0 = layer_0_2\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "                layer_2_2 = self.conv_2_2(x)\n",
        "                x = torch.cat((self.layer_2, layer_2_2), dim=1)\n",
        "                self.layer_2 = layer_2_2\n",
        "                x = F.relu(x, inplace=True)\n",
        "                x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "\n",
        "                layer_5_2 = self.conv_5_2(x)\n",
        "                x = torch.cat((self.layer_5, layer_5_2), dim=1)\n",
        "                self.layer_5 = layer_5_2\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                layer_7_2 = self.conv_7_2(x)\n",
        "                x = torch.cat((self.layer_7, layer_7_2), dim=1)\n",
        "                self.layer_7 = layer_7_2\n",
        "                x = F.relu(x, inplace=True)\n",
        "                x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "\n",
        "                layer_10_2 = self.conv_10_2(x)\n",
        "                x = torch.cat((self.layer_10, layer_10_2), dim=1)\n",
        "                self.layer_10 = layer_10_2\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                layer_12_2 = self.conv_12_2(x)\n",
        "                x = torch.cat((self.layer_12, layer_12_2), dim=1)\n",
        "                self.layer_12 = layer_12_2\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                layer_14_2 = self.conv_14_2(x)\n",
        "                x = torch.cat((self.layer_14, layer_14_2), dim=1)\n",
        "                self.layer_14  = layer_14_2\n",
        "                x = F.relu(x, inplace=True)\n",
        "                x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "\n",
        "                layer_17_2 = self.conv_17_2(x)\n",
        "                x = torch.cat((self.layer_17, layer_17_2), dim=1)\n",
        "                self.layer_17  = layer_17_2\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                layer_19_2 = self.conv_19_2(x)\n",
        "                x = torch.cat((self.layer_19, layer_19_2), dim=1)\n",
        "                self.layer_19 = layer_19_2\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                layer_21_2 = self.conv_21_2(x)\n",
        "                x = torch.cat((self.layer_21, layer_21_2), dim=1)\n",
        "                self.layer_21 = layer_21_2\n",
        "                x = F.relu(x, inplace=True)\n",
        "                x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "\n",
        "                layer_24_2 = self.conv_24_2(x)\n",
        "                x = torch.cat((self.layer_24, layer_24_2), dim=1)\n",
        "                self.layer_24 = layer_24_2\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                layer_26_2 = self.conv_26_2(x)\n",
        "                x = torch.cat((self.layer_26, layer_26_2), dim=1)\n",
        "                self.layer_26  = layer_26_2\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                layer_28_2 = self.conv_28_2(x)\n",
        "                x = torch.cat((self.layer_28, layer_28_2), dim=1)\n",
        "                self.layer_28 = layer_28_2\n",
        "                x = F.relu(x, inplace=True)\n",
        "                self.result = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "                \n",
        "            elif self.inf_checker == 2:\n",
        "                self.inf_checker -= 1\n",
        "                \n",
        "                layer_0_1 = self.conv_0_1(x)\n",
        "                x = torch.cat((layer_0_1, self.layer_0), dim=1)\n",
        "                self.layer_0 = layer_0_1\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "                layer_2_1 = self.conv_2_1(x)\n",
        "                x = torch.cat((layer_2_1, self.layer_2), dim=1)\n",
        "                self.layer_2 = layer_2_1\n",
        "                x = F.relu(x, inplace=True)\n",
        "                x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "\n",
        "                layer_5_1 = self.conv_5_1(x)\n",
        "                x = torch.cat((layer_5_1, self.layer_5), dim=1)\n",
        "                self.layer_5 = layer_5_1\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                layer_7_1 = self.conv_7_1(x)\n",
        "                x = torch.cat((layer_7_1, self.layer_7), dim=1)\n",
        "                self.layer_7 = layer_7_1\n",
        "                x = F.relu(x, inplace=True)\n",
        "                x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "\n",
        "                layer_10_1 = self.conv_10_1(x)\n",
        "                x = torch.cat((layer_10_1, self.layer_10), dim=1)\n",
        "                self.layer_10 = layer_10_1\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                layer_12_1 = self.conv_12_1(x)\n",
        "                x = torch.cat((layer_12_1, self.layer_12), dim=1)\n",
        "                self.layer_12 = layer_12_1\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                layer_14_1 = self.conv_14_1(x)\n",
        "                x = torch.cat((layer_14_1, self.layer_14), dim=1)\n",
        "                self.layer_14  = layer_14_1\n",
        "                x = F.relu(x, inplace=True)\n",
        "                x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "\n",
        "                layer_17_1 = self.conv_17_1(x)\n",
        "                x = torch.cat((layer_17_1, self.layer_17), dim=1)\n",
        "                self.layer_17  = layer_17_1\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                layer_19_1 = self.conv_19_1(x)\n",
        "                x = torch.cat((layer_19_1, self.layer_19), dim=1)\n",
        "                self.layer_19 = layer_19_1\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                layer_21_1 = self.conv_21_1(x)\n",
        "                x = torch.cat((layer_21_1, self.layer_21), dim=1)\n",
        "                self.layer_21 = layer_21_1\n",
        "                x = F.relu(x, inplace=True)\n",
        "                x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "\n",
        "                layer_24_1 = self.conv_24_1(x)\n",
        "                x = torch.cat((layer_24_1, self.layer_24), dim=1)\n",
        "                self.layer_24 = layer_24_1\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                layer_26_1 = self.conv_26_1(x)\n",
        "                x = torch.cat((layer_26_1, self.layer_26), dim=1)\n",
        "                self.layer_26  = layer_26_1\n",
        "                x = F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "                layer_28_1 = self.conv_28_1(x)\n",
        "                x = torch.cat((layer_28_1, self.layer_28), dim=1)\n",
        "                self.layer_28 = layer_28_1\n",
        "                x = F.relu(x, inplace=True)\n",
        "                self.result = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        else:\n",
        "            layer_0_1 = self.conv_0_1(x)\n",
        "            layer_0_2 = self.conv_0_2(x)\n",
        "            x = torch.cat((layer_0_1, layer_0_2), dim=1)\n",
        "            \n",
        "            x = F.relu(x, inplace=True)\n",
        "            \n",
        "            layer_2_1 = self.conv_2_1(x)\n",
        "            layer_2_2 = self.conv_2_2(x)\n",
        "            x = torch.cat((layer_2_1, layer_2_2), dim=1)\n",
        "            \n",
        "            x = F.relu(x, inplace=True)\n",
        "            x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "            \n",
        "            \n",
        "            layer_5_1 = self.conv_5_1(x)\n",
        "            layer_5_2 = self.conv_5_2(x)\n",
        "            x = torch.cat((layer_5_1, layer_5_2), dim=1)\n",
        "            \n",
        "            x = F.relu(x, inplace=True)\n",
        "            \n",
        "            \n",
        "            layer_7_1 = self.conv_7_1(x)\n",
        "            layer_7_2 = self.conv_7_2(x)\n",
        "            x = torch.cat((layer_7_1, layer_7_2), dim=1)\n",
        "            \n",
        "            x = F.relu(x, inplace=True)\n",
        "            x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "            \n",
        "            \n",
        "            layer_10_1 = self.conv_10_1(x)\n",
        "            layer_10_2 = self.conv_10_2(x)\n",
        "            x = torch.cat((layer_10_1, layer_10_2), dim=1)\n",
        "            \n",
        "            x = F.relu(x, inplace=True)\n",
        "            \n",
        "            \n",
        "            layer_12_1 = self.conv_12_1(x)\n",
        "            layer_12_2 = self.conv_12_2(x)\n",
        "            x = torch.cat((layer_12_1, layer_12_2), dim=1)\n",
        "            \n",
        "            x = F.relu(x, inplace=True)\n",
        "            \n",
        "            \n",
        "            layer_14_1 = self.conv_14_1(x)\n",
        "            layer_14_2 = self.conv_14_2(x)\n",
        "            x = torch.cat((layer_14_1, layer_14_2), dim=1)\n",
        "            \n",
        "            x = F.relu(x, inplace=True)\n",
        "            x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "            \n",
        "            \n",
        "            layer_17_1 = self.conv_17_1(x)\n",
        "            layer_17_2 = self.conv_17_2(x)\n",
        "            x = torch.cat((layer_17_1, layer_17_2), dim=1)\n",
        "            \n",
        "            x = F.relu(x, inplace=True)\n",
        "            \n",
        "            \n",
        "            layer_19_1 = self.conv_19_1(x)\n",
        "            layer_19_2 = self.conv_19_2(x)\n",
        "            x = torch.cat((layer_19_1, layer_19_2), dim=1)\n",
        "            \n",
        "            x = F.relu(x, inplace=True)\n",
        "            \n",
        "            \n",
        "            layer_21_1 = self.conv_21_1(x)\n",
        "            layer_21_2 = self.conv_21_2(x)\n",
        "            x = torch.cat((layer_21_1, layer_21_2), dim=1)\n",
        "            \n",
        "            x = F.relu(x, inplace=True)\n",
        "            x = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "            \n",
        "            \n",
        "            layer_24_1 = self.conv_24_1(x)\n",
        "            layer_24_2 = self.conv_24_2(x)\n",
        "            x = torch.cat((layer_24_1, layer_24_2), dim=1)\n",
        "            \n",
        "            x = F.relu(x, inplace=True)\n",
        "            \n",
        "            \n",
        "            layer_26_1 = self.conv_26_1(x)\n",
        "            layer_26_2 = self.conv_26_2(x)\n",
        "            x = torch.cat((layer_26_1, layer_26_2), dim=1)\n",
        "            \n",
        "            x = F.relu(x, inplace=True)\n",
        "            \n",
        "            \n",
        "            layer_28_1 = self.conv_28_1(x)\n",
        "            layer_28_2 = self.conv_28_2(x)\n",
        "            x = torch.cat((layer_28_1, layer_28_2), dim=1)\n",
        "            \n",
        "            x = F.relu(x, inplace=True)\n",
        "            self.result = F.max_pool2d(x, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "            \n",
        "            \n",
        "            \n",
        "        x = self.avgpool(self.result)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "id": "important-russia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "every-exhaust",
        "outputId": "0325fbcf-c28e-42c0-a967-4dc9c20ad4c7"
      },
      "source": [
        "network = models.vgg16(pretrained=True)\n",
        "network"
      ],
      "id": "every-exhaust",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "embedded-choir",
        "outputId": "01d2d50c-3e49-4abe-87eb-a7516c7f1f1c"
      },
      "source": [
        "network.features[0].bias"
      ],
      "id": "embedded-choir",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([ 0.4034,  0.3778,  0.4644, -0.3228,  0.3940, -0.3953,  0.3951, -0.5496,\n",
              "         0.2693, -0.7602, -0.3508,  0.2334, -1.3239, -0.1694,  0.3938, -0.1026,\n",
              "         0.0460, -0.6995,  0.1549,  0.5628,  0.3011,  0.3425,  0.1073,  0.4651,\n",
              "         0.1295,  0.0788, -0.0492, -0.5638,  0.1465, -0.3890, -0.0715,  0.0649,\n",
              "         0.2768,  0.3279,  0.5682, -1.2640, -0.8368, -0.9485,  0.1358,  0.2727,\n",
              "         0.1841, -0.5325,  0.3507, -0.0827, -1.0248, -0.6912, -0.7711,  0.2612,\n",
              "         0.4033, -0.4802, -0.3066,  0.5807, -1.3325,  0.4844, -0.8160,  0.2386,\n",
              "         0.2300,  0.4979,  0.5553,  0.5230, -0.2182,  0.0117, -0.5516,  0.2108],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voluntary-plate"
      },
      "source": [
        "network2 = VGG16_HALF()\n",
        "network2.inference_with_torchvision()"
      ],
      "id": "voluntary-plate",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hydraulic-subscriber",
        "outputId": "00b96a1e-541b-4407-db64-b859159a113b"
      },
      "source": [
        "network2.conv_0_2.bias"
      ],
      "id": "hydraulic-subscriber",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([ 0.2768,  0.3279,  0.5682, -1.2640, -0.8368, -0.9485,  0.1358,  0.2727,\n",
              "         0.1841, -0.5325,  0.3507, -0.0827, -1.0248, -0.6912, -0.7711,  0.2612,\n",
              "         0.4033, -0.4802, -0.3066,  0.5807, -1.3325,  0.4844, -0.8160,  0.2386,\n",
              "         0.2300,  0.4979,  0.5553,  0.5230, -0.2182,  0.0117, -0.5516,  0.2108],\n",
              "       grad_fn=<CopySlices>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d8e95fa"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "\n",
        "test_tensor = torch.rand(1, 3, 244, 244)"
      ],
      "id": "4d8e95fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4813c3e1"
      },
      "source": [
        "test_tensor = test_tensor.to(DEVICE)\n",
        "network = network.to(DEVICE).eval()\n",
        "network2 = network2.to(DEVICE).eval()"
      ],
      "id": "4813c3e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7c14d1e"
      },
      "source": [
        "import time\n",
        "\n",
        "def eval(model, tensor):\n",
        "    time_ = time.time()\n",
        "    output = None\n",
        "    with torch.no_grad():\n",
        "        time_ -= time_\n",
        "        for i in range(10000):\n",
        "            start = time.time()  # 시작 시간 저장\n",
        "            output = model(tensor)\n",
        "            end = time.time() - start\n",
        "            time_ += end\n",
        "    print(output)\n",
        "    print(\"time :\", time_)  # 현재시각 - 시작시간 = 실행 시간\n",
        "    return"
      ],
      "id": "c7c14d1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed0bcbcd",
        "outputId": "04053183-b378-4262-cf5b-37315a4b0cb4"
      },
      "source": [
        "eval(network, test_tensor)"
      ],
      "id": "ed0bcbcd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-7.4385e-01,  1.4804e+00, -5.3962e-01,  5.4345e-01, -4.1696e-01,\n",
            "         -1.8613e-01, -6.0308e-01, -4.6140e-01, -5.9823e-01,  8.8839e-03,\n",
            "          4.5382e-01,  1.2223e+00,  9.3680e-01,  1.0030e+00,  7.3924e-01,\n",
            "          5.4289e-01,  2.3086e-01, -2.5446e-01,  1.1991e-01,  8.6787e-01,\n",
            "          2.4375e-01,  6.2169e-01,  4.5873e-02, -3.5269e-01,  4.0353e-01,\n",
            "         -6.8085e-01,  7.5815e-01, -1.4649e-03, -1.1122e+00,  2.7171e-01,\n",
            "         -1.1803e+00, -5.9074e-02, -4.6356e-01, -4.3567e-01, -3.3285e-01,\n",
            "         -8.0615e-01,  2.6849e-01, -1.7941e+00,  4.1561e-01, -1.3428e+00,\n",
            "         -1.1536e-02, -3.6764e-01, -1.6046e-01,  1.2216e-01, -6.0874e-01,\n",
            "         -7.0940e-01, -2.6716e-01, -3.0937e-01, -1.1674e+00, -2.4726e-01,\n",
            "          5.9597e-01, -1.2327e+00,  2.5826e-01,  5.8163e-01, -1.1394e-01,\n",
            "         -4.9212e-01, -4.9626e-01, -1.1175e+00,  1.0810e+00,  4.5191e-01,\n",
            "         -1.1261e-01, -1.3682e+00, -7.1520e-01,  8.7955e-01,  7.0190e-01,\n",
            "          4.8610e-01,  2.2557e-02, -4.0715e-01, -4.1010e-01,  1.0453e-01,\n",
            "          2.6733e-01,  2.4213e-01,  1.0820e+00,  2.1129e+00,  5.0970e-01,\n",
            "          1.8633e+00, -8.3764e-02,  1.3136e+00,  1.6341e+00,  1.7486e+00,\n",
            "          1.5863e+00,  3.2545e-01, -9.1828e-02,  1.3887e+00, -6.4163e-01,\n",
            "          8.5933e-01,  5.4783e-01,  1.1240e+00, -1.3046e-01, -8.5279e-01,\n",
            "         -7.4634e-01,  2.7560e-01,  9.3578e-01, -5.3383e-01,  4.6995e-01,\n",
            "         -9.9149e-02, -2.8899e-01, -7.5107e-01,  4.3313e-01, -2.7696e-01,\n",
            "          3.5376e-01, -1.1748e+00, -1.3980e+00,  1.8103e+00, -7.5392e-01,\n",
            "         -1.5799e+00, -6.6526e-01,  2.2054e-01, -1.0307e+00, -1.6654e+00,\n",
            "         -1.8539e-01,  3.4019e+00,  3.6206e-01, -1.3780e-01, -5.1835e-01,\n",
            "         -6.4013e-01, -1.3116e+00,  4.3284e-01, -1.3119e+00, -1.6515e+00,\n",
            "         -5.7537e-01, -1.7243e+00, -9.3877e-01, -1.2554e+00, -7.0863e-01,\n",
            "         -1.2459e+00,  9.4431e-01, -5.7970e-01,  7.3166e-01, -4.8994e-01,\n",
            "         -3.2050e-01,  7.6387e-02,  4.7053e-01,  3.4763e-01,  1.1433e-01,\n",
            "         -2.8045e-01, -2.2304e-01,  6.5024e-01,  9.7364e-01, -7.5421e-02,\n",
            "         -5.5115e-01,  1.1297e+00,  1.7079e-01, -2.6345e-01, -4.1528e-01,\n",
            "          3.7342e-01, -4.0163e-01,  1.9580e-01, -6.0910e-01, -1.3388e-02,\n",
            "         -9.9836e-04, -1.1082e-01, -3.2525e-01,  7.0204e-01, -5.1443e-02,\n",
            "          3.3909e-01, -6.7056e-02, -2.7562e-02, -2.2384e-01, -5.7282e-01,\n",
            "          2.0210e-01,  1.6263e-01, -9.6562e-02,  3.6492e-01,  2.2468e-03,\n",
            "         -1.5999e-01,  2.0206e-01, -4.2065e-01, -2.0763e-01, -1.0544e-02,\n",
            "         -3.4059e-01,  1.8549e-01, -2.1002e-01, -4.1591e-01, -4.3183e-01,\n",
            "         -1.7493e-01, -8.2035e-01, -1.8623e-02, -3.3839e-01, -8.0877e-01,\n",
            "         -7.8285e-01, -2.3067e-01, -9.1904e-02,  2.4656e-01,  2.3014e-01,\n",
            "         -3.7154e-01, -2.3701e-01,  4.2178e-01,  5.7082e-02, -3.3908e-01,\n",
            "          1.5777e-01,  4.3404e-01, -2.1312e-01,  2.1297e-01,  2.7286e-01,\n",
            "         -2.0891e-01,  6.4960e-02, -4.1091e-01, -3.4761e-01, -2.0758e-02,\n",
            "         -6.6600e-02, -4.5688e-01,  3.4935e-01,  7.4217e-01,  3.6380e-02,\n",
            "         -5.2469e-01, -5.6685e-01, -2.3823e-01, -6.5064e-01, -2.1959e-01,\n",
            "          2.5911e-01, -2.2888e-01,  4.0978e-01, -4.3934e-01, -2.6504e-01,\n",
            "          3.7995e-02, -2.3965e-01,  4.1477e-01, -3.8380e-01, -3.7248e-01,\n",
            "         -3.3986e-01,  5.1972e-02, -4.8214e-01, -4.1012e-02, -8.6319e-02,\n",
            "         -5.4240e-01, -7.1136e-02, -3.9521e-01,  1.0748e-01,  4.7852e-01,\n",
            "         -3.4945e-02, -4.6166e-01, -2.2893e-01, -2.1084e-01, -2.6981e-01,\n",
            "         -4.5201e-01,  3.3378e-02,  4.3637e-02, -4.5892e-01,  1.8745e-01,\n",
            "         -4.4406e-01,  6.0613e-02,  4.4952e-02, -6.4187e-01, -1.4272e+00,\n",
            "         -1.6164e-01, -6.0419e-01, -4.8529e-01, -5.0862e-01, -9.9468e-02,\n",
            "         -7.5788e-02, -3.0424e-01,  2.9388e-02, -3.3757e-01,  2.6532e-01,\n",
            "         -9.6890e-01, -3.6888e-01, -2.9820e-01,  3.3749e-01,  1.1886e-01,\n",
            "         -2.1230e-01, -1.5888e-01, -6.3335e-01, -1.1734e-01, -4.0291e-01,\n",
            "          2.4844e-01, -5.5545e-01, -6.5205e-01, -1.0902e+00,  3.6591e-02,\n",
            "          4.1445e-01, -1.8908e-01,  4.6877e-01,  3.7681e-01,  3.5081e-01,\n",
            "         -1.6425e-01, -9.2678e-02,  3.8070e-01, -2.2806e-02,  6.5359e-01,\n",
            "          4.9850e-01,  8.0728e-01,  1.1169e+00,  1.1384e+00,  6.8824e-01,\n",
            "          9.5939e-01, -3.4475e-01, -7.0813e-02, -7.1439e-01, -6.8659e-01,\n",
            "         -1.3461e+00, -8.3473e-01, -1.6688e-01,  1.3983e-01, -4.8327e-01,\n",
            "         -7.3752e-01,  4.8660e-01, -2.0548e-02,  5.7182e-01,  9.1997e-01,\n",
            "         -8.8592e-01,  6.0342e-01,  1.1056e-01, -2.8971e-01, -3.4949e-01,\n",
            "         -1.2913e+00, -7.7606e-01,  2.5860e-01,  4.2592e-01, -9.6248e-01,\n",
            "          1.0391e+00,  2.6247e-01,  1.9228e-01,  5.4780e-01,  8.0445e-01,\n",
            "         -9.2695e-02,  1.3298e-01, -1.2238e-02,  1.6392e+00,  1.0296e+00,\n",
            "          8.2907e-01, -1.4700e+00, -5.9922e-01, -1.2784e+00, -1.1546e+00,\n",
            "         -1.3563e+00, -3.4128e-01, -3.2267e-01, -1.3861e+00, -1.4859e+00,\n",
            "         -1.3301e-01,  6.7848e-01,  4.6709e-01, -5.6896e-01, -5.3764e-01,\n",
            "          9.8048e-02, -1.0103e+00,  9.8881e-01, -9.7044e-01, -1.5368e+00,\n",
            "         -4.6683e-01, -1.4186e+00, -1.3591e+00, -1.3577e+00, -6.4267e-01,\n",
            "         -1.2442e+00, -1.4280e+00, -8.0808e-01, -1.4790e+00, -8.6885e-01,\n",
            "         -7.4794e-01,  1.2914e-01, -1.4433e-01,  1.2823e-01, -2.3973e-01,\n",
            "         -7.3057e-01,  6.0422e-01,  2.7698e-01,  1.4321e-01,  7.5630e-01,\n",
            "          3.5549e-01,  1.6275e-01,  1.6276e-01, -2.7816e-01, -8.6404e-01,\n",
            "         -5.0997e-01, -2.5736e-01, -3.2262e-01, -4.8008e-01, -5.7317e-01,\n",
            "         -7.6263e-02,  5.3617e-01, -6.1445e-01, -7.7106e-01,  4.4485e-01,\n",
            "         -9.7659e-01, -9.3140e-04,  4.3836e-01, -6.1316e-01, -1.0477e+00,\n",
            "          1.9348e-02, -9.9145e-01, -4.6660e-01,  2.5218e-01, -3.5016e-01,\n",
            "         -9.9596e-01, -1.2644e+00, -8.5284e-01, -9.7842e-01,  7.1816e-01,\n",
            "         -4.9781e-01,  1.3865e-02, -1.8117e-01, -1.3819e+00, -3.6781e-01,\n",
            "          1.4797e+00, -8.2545e-01, -2.9626e-01, -8.5989e-01,  1.2948e+00,\n",
            "         -3.0467e-01, -1.0858e+00,  7.3612e-01, -6.1273e-01,  7.1874e-01,\n",
            "          1.1337e+00, -2.0877e-01, -9.7329e-01, -1.8363e+00,  8.0424e-01,\n",
            "         -1.7196e+00,  2.0346e-01,  1.3009e+00,  4.6415e-01, -8.9180e-01,\n",
            "         -1.8896e+00,  1.0088e+00,  8.9262e-01,  2.2112e+00,  1.5731e-01,\n",
            "          7.1490e-01,  1.9452e-01,  5.6215e-02, -1.1290e+00, -1.5752e+00,\n",
            "         -3.8754e-01,  6.8447e-01,  5.0278e-01, -6.9443e-01,  3.8280e-02,\n",
            "         -2.5583e-01, -2.5136e-02,  1.2283e+00,  7.7656e-01,  9.1943e-01,\n",
            "          1.4152e+00, -1.2351e+00,  8.9282e-01,  1.8853e+00,  1.2512e-01,\n",
            "          6.2750e-01,  1.8391e+00, -5.2879e-01,  6.4196e-01, -7.4661e-01,\n",
            "         -6.9332e-01,  2.4011e+00,  2.7346e-02,  4.7083e-01, -7.2789e-01,\n",
            "         -3.4903e-01,  1.1220e+00,  2.2001e-01,  7.7738e-03, -1.6467e+00,\n",
            "          1.9680e-01,  6.0655e-01,  1.7157e-02, -9.3304e-02,  3.5859e-01,\n",
            "         -3.3110e-03,  9.6267e-01,  2.6110e+00,  6.8646e-01,  1.4829e-03,\n",
            "          3.6276e-01, -7.0912e-01, -2.3556e+00, -7.2689e-01,  1.0799e-01,\n",
            "          2.3207e+00, -5.9425e-01, -8.0462e-01,  1.1903e+00, -1.5101e-01,\n",
            "         -4.7016e-01, -1.8712e+00, -2.4973e-01,  8.2522e-01, -1.1223e+00,\n",
            "          8.2031e-01,  6.7115e-01, -3.8919e-01, -1.5631e+00, -7.2021e-02,\n",
            "          9.5390e-01,  3.5545e-01,  9.2818e-01,  4.6911e-01, -1.1909e+00,\n",
            "          4.5322e-01,  1.6245e-01,  4.6822e-01,  8.9126e-01,  1.8744e+00,\n",
            "          1.3984e-01, -2.1865e-01, -6.8005e-01, -7.4288e-01,  9.4058e-01,\n",
            "         -1.5988e+00,  1.2811e+00, -7.8907e-01,  1.6911e+00,  9.9028e-01,\n",
            "          1.1185e+00, -2.7788e-01,  9.5543e-01, -5.0556e-01, -2.1603e+00,\n",
            "         -1.2165e-01, -1.3796e+00,  5.5024e-01, -2.2459e-02,  2.3303e-01,\n",
            "          2.2832e-01,  6.7467e-03,  7.7231e-01,  4.2846e-03,  3.4620e-01,\n",
            "          3.9393e-01,  4.7336e-01, -4.1727e-01,  8.5216e-01,  2.0950e-01,\n",
            "         -1.0080e+00,  1.8271e-01,  8.0096e-01, -4.5536e-01, -7.5170e-01,\n",
            "          4.4042e-01, -1.3114e-01,  8.9067e-01, -1.0677e-01,  2.2415e+00,\n",
            "         -1.4101e+00, -9.0886e-01, -7.9302e-01, -3.5247e-01,  5.3886e-01,\n",
            "          1.3160e+00, -6.8935e-02,  7.7891e-01,  2.1373e-02, -6.6458e-01,\n",
            "          1.4832e+00,  3.3633e-01, -1.5087e+00,  4.6308e-01,  1.3206e+00,\n",
            "         -3.6630e-01, -1.5998e-01,  2.8342e-01,  1.7599e+00,  3.5121e-01,\n",
            "         -1.1614e+00,  2.4830e+00,  2.4324e-01,  1.1386e+00,  9.5402e-01,\n",
            "         -4.6546e-01, -6.9102e-01, -1.8885e-02,  9.5421e-01, -8.4552e-03,\n",
            "          1.0073e-01, -4.7235e-02,  1.9576e-01, -2.0663e-01, -1.5672e+00,\n",
            "          1.8775e-01, -1.0667e+00,  1.7751e+00, -1.0125e+00,  4.7895e-01,\n",
            "         -1.4526e+00, -8.1817e-01,  6.7103e-01,  1.0334e+00, -5.7445e-01,\n",
            "         -1.5068e+00, -1.7942e+00, -2.4605e+00,  1.3047e+00,  7.0822e-01,\n",
            "          1.7820e+00, -1.1579e+00,  9.6956e-01,  1.3970e+00,  8.8601e-01,\n",
            "         -5.6285e-01,  4.4243e-01,  8.7033e-01,  8.0565e-01,  2.2643e-01,\n",
            "         -1.1304e+00,  8.6093e-01, -3.1113e-01,  8.8712e-01, -4.9797e-01,\n",
            "          1.9540e+00,  1.0573e+00,  3.4215e-01, -1.9290e+00,  1.5963e+00,\n",
            "          8.6718e-01,  8.6931e-01, -4.7695e-01, -2.8006e-01, -1.2497e+00,\n",
            "          9.4932e-01,  2.6263e-02, -1.6256e+00,  9.6058e-01, -5.5998e-01,\n",
            "          2.2288e-01,  9.8320e-01,  2.8443e-01,  1.8558e+00,  2.6383e+00,\n",
            "          1.2488e+00, -5.7580e-01,  4.1469e-01,  1.2508e+00, -1.3232e+00,\n",
            "         -1.1588e-01,  1.7519e+00, -1.0899e+00,  3.4316e-01,  1.4987e+00,\n",
            "         -9.8150e-01,  1.0027e+00,  1.6948e+00,  3.6158e-01, -2.0132e+00,\n",
            "         -3.6196e-01,  1.9650e-01, -4.3778e-01, -3.9107e-02, -1.4330e-01,\n",
            "         -1.0730e+00,  4.1350e-01, -2.5011e-01,  1.0239e-01,  1.7290e+00,\n",
            "         -1.1522e+00,  2.1704e-01,  1.5222e+00,  2.1593e+00, -1.2997e-01,\n",
            "          1.0871e+00,  1.3845e+00, -1.0878e+00,  1.6103e+00, -1.5905e+00,\n",
            "          5.7137e-01, -1.8938e+00,  1.2965e+00, -1.0498e+00,  6.6904e-01,\n",
            "         -8.8675e-01, -1.9927e+00,  2.2317e+00, -9.2451e-01,  1.5939e+00,\n",
            "         -8.7393e-01,  3.0570e-01,  5.5012e-02, -5.7749e-01,  1.6918e+00,\n",
            "         -2.7591e-01, -7.1415e-01, -6.8740e-01,  4.6418e-01,  6.4874e-01,\n",
            "         -1.1506e+00, -6.2630e-01,  1.5423e+00,  1.5662e-01, -1.1310e-01,\n",
            "          1.8369e+00,  9.2649e-01,  6.0066e-01,  1.6356e+00,  7.4164e-01,\n",
            "         -7.5941e-01,  1.7149e+00, -9.0309e-01,  9.5444e-01,  6.3491e-01,\n",
            "         -2.9188e+00,  5.0774e-01,  2.3106e-01,  7.3993e-01, -1.0757e+00,\n",
            "          5.9230e-01,  1.0650e+00,  1.5066e-01, -1.8407e+00,  1.3804e+00,\n",
            "          1.8099e+00,  9.2607e-01,  5.5700e-01, -1.2212e+00, -2.4540e-01,\n",
            "         -1.1889e+00, -1.2615e+00, -3.2769e-01,  1.8969e+00, -3.9573e-01,\n",
            "          7.6886e-01,  1.4737e+00,  8.5087e-01,  1.3206e+00,  6.7492e-01,\n",
            "          7.3682e-01, -1.4192e-01, -1.8183e+00,  3.3392e-01,  1.3612e-03,\n",
            "          7.0200e-01, -3.1268e-01,  1.1951e+00, -4.7830e-01, -6.6962e-01,\n",
            "          1.3985e+00, -8.6218e-02,  1.5663e-02,  1.0044e+00,  5.2000e-01,\n",
            "         -7.4575e-01,  2.5688e+00,  8.7633e-02,  1.5797e+00, -1.1691e+00,\n",
            "          4.9038e-01,  4.6673e-01,  1.3116e+00,  2.8522e-01,  1.0150e-01,\n",
            "          3.2659e-01,  6.7305e-01,  1.9768e-01, -4.8511e-02,  1.8242e+00,\n",
            "          1.5330e+00,  4.4466e-01,  9.6605e-01,  2.0891e-01,  8.7401e-01,\n",
            "         -1.7572e-01, -4.4288e-01,  9.8908e-01,  2.2995e+00,  3.2932e-01,\n",
            "         -3.3575e-01,  5.4761e-01, -4.1516e-01,  4.4000e-03, -1.0185e-02,\n",
            "          1.7081e+00,  7.4811e-01, -1.1678e+00,  6.8664e-01,  7.2794e-01,\n",
            "          7.1041e-02, -1.3011e+00,  7.8389e-01, -8.7812e-01,  1.3461e+00,\n",
            "         -1.0163e+00,  1.7212e+00,  3.9043e-01,  9.6227e-01, -8.3254e-01,\n",
            "          4.5086e-01,  5.1856e-01,  9.8015e-01,  3.4401e-01, -1.1470e+00,\n",
            "          2.5996e-01, -4.6494e-01,  1.5802e+00,  1.6867e+00,  1.6425e+00,\n",
            "         -1.2626e+00,  1.7484e-01,  1.3488e-01, -1.7402e+00,  1.2969e+00,\n",
            "         -3.7401e-01,  4.0341e-01,  7.8936e-01,  1.1552e+00,  2.4409e+00,\n",
            "          5.4018e-01,  6.4766e-01,  1.4521e-01,  3.8361e-01,  1.6536e+00,\n",
            "         -1.8584e+00,  4.7237e-01, -1.8005e-01, -3.3551e-01,  2.4040e+00,\n",
            "         -9.0665e-01, -5.7709e-01,  5.8381e-01,  1.5959e-01, -3.5200e-01,\n",
            "         -2.9257e-01,  1.5517e+00,  1.6036e+00,  1.1049e+00, -3.0659e-01,\n",
            "          8.7755e-01,  1.7404e-01, -5.5050e-01,  2.1968e+00, -5.4800e-02,\n",
            "         -1.0245e+00, -9.0937e-01, -4.2242e-01, -3.5559e-01,  2.3411e-01,\n",
            "         -1.8961e+00, -3.2425e-02,  1.0025e+00,  6.2592e-01, -1.3327e+00,\n",
            "          2.1677e-01,  4.5579e-01, -8.3488e-01,  7.1169e-01,  6.1742e-01,\n",
            "         -1.2807e+00, -7.5230e-01, -2.8575e-01,  4.2627e-01, -1.1519e-01,\n",
            "          1.7113e+00, -3.6324e-01,  6.2965e-01, -1.1591e+00,  2.1348e+00,\n",
            "          1.4781e+00,  2.6737e+00, -1.1703e+00,  2.3814e-01,  1.5739e-01,\n",
            "         -7.2363e-01,  1.9570e+00,  1.3477e+00, -1.6697e+00,  6.8577e-01,\n",
            "          1.9864e+00, -1.6415e+00, -1.7791e-01, -1.3468e+00,  9.1334e-01,\n",
            "         -7.8481e-01,  1.6235e+00,  1.0686e+00, -9.7002e-01, -1.3965e+00,\n",
            "         -1.6919e+00, -9.0862e-01, -4.6273e-01,  1.0658e+00,  1.2017e-01,\n",
            "         -3.2849e-01,  4.4676e-02,  1.3644e+00, -8.7457e-01, -1.5334e+00,\n",
            "          1.7887e-01,  1.4287e+00, -3.7701e-01, -1.3136e+00, -1.4604e-01,\n",
            "          5.1751e-01, -1.0399e-01,  1.2479e+00,  2.3397e+00,  3.6733e-01,\n",
            "          1.0917e+00,  1.1190e-02, -4.7464e-02, -1.2932e+00,  3.1999e-01,\n",
            "         -7.7405e-01, -4.0441e-01,  2.2218e+00,  4.5132e-01,  2.0220e+00,\n",
            "         -1.0405e-01,  1.7415e+00,  1.6375e+00,  1.2847e+00,  1.7764e+00,\n",
            "          6.0493e-01,  1.8856e+00,  1.2546e+00,  7.3737e-01,  1.5125e+00,\n",
            "          1.3100e+00,  5.1960e-01,  6.8177e-01,  5.6165e-01,  1.0763e-01,\n",
            "          3.2425e-01,  8.3909e-01, -6.1835e-01, -5.0002e-02,  4.3011e-01,\n",
            "         -1.2309e+00, -5.7340e-01, -9.5635e-01, -9.9611e-01,  1.9160e-01,\n",
            "          4.4012e-01,  2.0608e+00, -2.2777e-01, -6.0421e-01, -1.9652e+00,\n",
            "          2.5480e-01, -1.0116e+00, -1.1112e+00, -1.6990e+00,  1.1059e-01,\n",
            "         -1.1693e+00, -1.6786e+00, -1.4337e+00, -2.0690e+00, -1.4222e+00,\n",
            "         -1.1874e+00, -1.2686e+00, -1.3890e+00, -1.7546e+00, -8.3735e-01,\n",
            "         -1.9694e-01, -4.7911e-01, -2.5977e-01, -4.0726e-01, -1.9306e+00,\n",
            "         -3.7464e-01, -1.2813e+00, -1.3014e+00,  3.3303e-01, -4.5705e-01,\n",
            "         -6.7607e-01, -2.5896e-01, -1.0624e+00, -8.0823e-01, -9.9576e-01,\n",
            "         -2.0364e+00, -1.9078e+00, -1.1812e+00, -2.4760e-01, -1.2174e+00,\n",
            "         -1.0250e+00, -9.6908e-01, -1.5964e+00, -1.8124e+00, -1.4917e+00,\n",
            "         -1.4575e+00,  6.8099e-01, -1.9782e-01,  7.2319e-01,  2.1947e-01,\n",
            "         -6.3504e-01,  9.0749e-01, -9.3020e-01, -1.1633e+00,  1.1875e+00,\n",
            "          1.0996e-01, -3.0598e-01,  5.9397e-01,  3.5554e-01, -1.0320e+00,\n",
            "          4.5852e-01, -1.7264e-02, -9.9292e-01, -5.6485e-01, -8.3827e-01,\n",
            "         -1.5039e+00, -1.2357e+00, -6.4257e-01, -7.2143e-01, -1.2031e-01,\n",
            "         -1.8690e+00, -1.3193e+00, -1.1603e+00, -1.9892e+00, -7.8842e-01,\n",
            "         -9.6700e-01, -1.5960e+00, -1.9164e+00, -1.6558e-02,  1.5575e+00]],\n",
            "       device='cuda:0')\n",
            "time : 98.06857252120972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be337494",
        "outputId": "037c2c51-b173-4b7a-e1cc-0b713611c5bb"
      },
      "source": [
        "network2.change_status()\n",
        "eval(network2, test_tensor)"
      ],
      "id": "be337494",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "tensor([[-7.4385e-01,  1.4804e+00, -5.3962e-01,  5.4345e-01, -4.1696e-01,\n",
            "         -1.8613e-01, -6.0308e-01, -4.6140e-01, -5.9823e-01,  8.8838e-03,\n",
            "          4.5382e-01,  1.2223e+00,  9.3680e-01,  1.0030e+00,  7.3924e-01,\n",
            "          5.4289e-01,  2.3086e-01, -2.5446e-01,  1.1991e-01,  8.6787e-01,\n",
            "          2.4375e-01,  6.2169e-01,  4.5873e-02, -3.5269e-01,  4.0353e-01,\n",
            "         -6.8085e-01,  7.5815e-01, -1.4653e-03, -1.1122e+00,  2.7171e-01,\n",
            "         -1.1803e+00, -5.9074e-02, -4.6356e-01, -4.3567e-01, -3.3285e-01,\n",
            "         -8.0615e-01,  2.6849e-01, -1.7941e+00,  4.1561e-01, -1.3428e+00,\n",
            "         -1.1537e-02, -3.6764e-01, -1.6046e-01,  1.2216e-01, -6.0874e-01,\n",
            "         -7.0940e-01, -2.6716e-01, -3.0937e-01, -1.1674e+00, -2.4726e-01,\n",
            "          5.9597e-01, -1.2327e+00,  2.5826e-01,  5.8163e-01, -1.1394e-01,\n",
            "         -4.9212e-01, -4.9626e-01, -1.1175e+00,  1.0810e+00,  4.5191e-01,\n",
            "         -1.1261e-01, -1.3682e+00, -7.1520e-01,  8.7955e-01,  7.0190e-01,\n",
            "          4.8610e-01,  2.2557e-02, -4.0715e-01, -4.1010e-01,  1.0453e-01,\n",
            "          2.6733e-01,  2.4213e-01,  1.0820e+00,  2.1129e+00,  5.0970e-01,\n",
            "          1.8633e+00, -8.3764e-02,  1.3136e+00,  1.6341e+00,  1.7486e+00,\n",
            "          1.5863e+00,  3.2545e-01, -9.1828e-02,  1.3887e+00, -6.4163e-01,\n",
            "          8.5933e-01,  5.4783e-01,  1.1240e+00, -1.3046e-01, -8.5279e-01,\n",
            "         -7.4634e-01,  2.7560e-01,  9.3578e-01, -5.3383e-01,  4.6995e-01,\n",
            "         -9.9149e-02, -2.8899e-01, -7.5107e-01,  4.3313e-01, -2.7696e-01,\n",
            "          3.5376e-01, -1.1748e+00, -1.3980e+00,  1.8103e+00, -7.5392e-01,\n",
            "         -1.5799e+00, -6.6526e-01,  2.2055e-01, -1.0307e+00, -1.6654e+00,\n",
            "         -1.8539e-01,  3.4019e+00,  3.6206e-01, -1.3780e-01, -5.1835e-01,\n",
            "         -6.4013e-01, -1.3116e+00,  4.3284e-01, -1.3119e+00, -1.6515e+00,\n",
            "         -5.7537e-01, -1.7243e+00, -9.3877e-01, -1.2554e+00, -7.0863e-01,\n",
            "         -1.2459e+00,  9.4431e-01, -5.7970e-01,  7.3166e-01, -4.8994e-01,\n",
            "         -3.2050e-01,  7.6387e-02,  4.7053e-01,  3.4763e-01,  1.1433e-01,\n",
            "         -2.8045e-01, -2.2304e-01,  6.5024e-01,  9.7364e-01, -7.5421e-02,\n",
            "         -5.5115e-01,  1.1297e+00,  1.7079e-01, -2.6345e-01, -4.1528e-01,\n",
            "          3.7342e-01, -4.0163e-01,  1.9580e-01, -6.0910e-01, -1.3388e-02,\n",
            "         -9.9800e-04, -1.1082e-01, -3.2525e-01,  7.0204e-01, -5.1442e-02,\n",
            "          3.3909e-01, -6.7056e-02, -2.7561e-02, -2.2384e-01, -5.7282e-01,\n",
            "          2.0210e-01,  1.6263e-01, -9.6562e-02,  3.6492e-01,  2.2467e-03,\n",
            "         -1.5999e-01,  2.0206e-01, -4.2065e-01, -2.0763e-01, -1.0544e-02,\n",
            "         -3.4059e-01,  1.8549e-01, -2.1002e-01, -4.1591e-01, -4.3183e-01,\n",
            "         -1.7493e-01, -8.2035e-01, -1.8623e-02, -3.3839e-01, -8.0877e-01,\n",
            "         -7.8285e-01, -2.3067e-01, -9.1903e-02,  2.4656e-01,  2.3014e-01,\n",
            "         -3.7154e-01, -2.3701e-01,  4.2178e-01,  5.7082e-02, -3.3908e-01,\n",
            "          1.5777e-01,  4.3404e-01, -2.1312e-01,  2.1297e-01,  2.7286e-01,\n",
            "         -2.0891e-01,  6.4960e-02, -4.1091e-01, -3.4761e-01, -2.0758e-02,\n",
            "         -6.6599e-02, -4.5688e-01,  3.4935e-01,  7.4217e-01,  3.6380e-02,\n",
            "         -5.2469e-01, -5.6685e-01, -2.3823e-01, -6.5064e-01, -2.1959e-01,\n",
            "          2.5911e-01, -2.2888e-01,  4.0978e-01, -4.3934e-01, -2.6504e-01,\n",
            "          3.7995e-02, -2.3965e-01,  4.1477e-01, -3.8380e-01, -3.7248e-01,\n",
            "         -3.3986e-01,  5.1973e-02, -4.8214e-01, -4.1012e-02, -8.6318e-02,\n",
            "         -5.4240e-01, -7.1135e-02, -3.9521e-01,  1.0748e-01,  4.7852e-01,\n",
            "         -3.4945e-02, -4.6166e-01, -2.2893e-01, -2.1084e-01, -2.6981e-01,\n",
            "         -4.5201e-01,  3.3378e-02,  4.3637e-02, -4.5892e-01,  1.8745e-01,\n",
            "         -4.4406e-01,  6.0613e-02,  4.4952e-02, -6.4187e-01, -1.4272e+00,\n",
            "         -1.6164e-01, -6.0419e-01, -4.8529e-01, -5.0862e-01, -9.9468e-02,\n",
            "         -7.5788e-02, -3.0424e-01,  2.9389e-02, -3.3757e-01,  2.6532e-01,\n",
            "         -9.6890e-01, -3.6888e-01, -2.9820e-01,  3.3749e-01,  1.1886e-01,\n",
            "         -2.1230e-01, -1.5888e-01, -6.3335e-01, -1.1734e-01, -4.0291e-01,\n",
            "          2.4844e-01, -5.5545e-01, -6.5205e-01, -1.0902e+00,  3.6591e-02,\n",
            "          4.1445e-01, -1.8908e-01,  4.6877e-01,  3.7681e-01,  3.5081e-01,\n",
            "         -1.6425e-01, -9.2678e-02,  3.8070e-01, -2.2806e-02,  6.5359e-01,\n",
            "          4.9850e-01,  8.0728e-01,  1.1169e+00,  1.1384e+00,  6.8824e-01,\n",
            "          9.5939e-01, -3.4475e-01, -7.0813e-02, -7.1439e-01, -6.8659e-01,\n",
            "         -1.3462e+00, -8.3473e-01, -1.6688e-01,  1.3983e-01, -4.8327e-01,\n",
            "         -7.3752e-01,  4.8660e-01, -2.0548e-02,  5.7182e-01,  9.1997e-01,\n",
            "         -8.8592e-01,  6.0342e-01,  1.1056e-01, -2.8971e-01, -3.4949e-01,\n",
            "         -1.2913e+00, -7.7607e-01,  2.5860e-01,  4.2592e-01, -9.6248e-01,\n",
            "          1.0391e+00,  2.6247e-01,  1.9228e-01,  5.4780e-01,  8.0445e-01,\n",
            "         -9.2695e-02,  1.3298e-01, -1.2238e-02,  1.6392e+00,  1.0296e+00,\n",
            "          8.2907e-01, -1.4700e+00, -5.9922e-01, -1.2784e+00, -1.1546e+00,\n",
            "         -1.3563e+00, -3.4128e-01, -3.2267e-01, -1.3861e+00, -1.4859e+00,\n",
            "         -1.3301e-01,  6.7848e-01,  4.6709e-01, -5.6896e-01, -5.3764e-01,\n",
            "          9.8049e-02, -1.0103e+00,  9.8881e-01, -9.7044e-01, -1.5368e+00,\n",
            "         -4.6683e-01, -1.4186e+00, -1.3591e+00, -1.3577e+00, -6.4267e-01,\n",
            "         -1.2442e+00, -1.4280e+00, -8.0808e-01, -1.4790e+00, -8.6885e-01,\n",
            "         -7.4794e-01,  1.2914e-01, -1.4433e-01,  1.2823e-01, -2.3973e-01,\n",
            "         -7.3057e-01,  6.0422e-01,  2.7698e-01,  1.4321e-01,  7.5630e-01,\n",
            "          3.5549e-01,  1.6275e-01,  1.6276e-01, -2.7816e-01, -8.6404e-01,\n",
            "         -5.0997e-01, -2.5736e-01, -3.2262e-01, -4.8008e-01, -5.7317e-01,\n",
            "         -7.6263e-02,  5.3617e-01, -6.1445e-01, -7.7106e-01,  4.4485e-01,\n",
            "         -9.7659e-01, -9.3122e-04,  4.3836e-01, -6.1316e-01, -1.0477e+00,\n",
            "          1.9348e-02, -9.9145e-01, -4.6660e-01,  2.5218e-01, -3.5016e-01,\n",
            "         -9.9596e-01, -1.2644e+00, -8.5284e-01, -9.7842e-01,  7.1816e-01,\n",
            "         -4.9781e-01,  1.3865e-02, -1.8117e-01, -1.3819e+00, -3.6781e-01,\n",
            "          1.4797e+00, -8.2545e-01, -2.9626e-01, -8.5989e-01,  1.2948e+00,\n",
            "         -3.0467e-01, -1.0858e+00,  7.3612e-01, -6.1273e-01,  7.1873e-01,\n",
            "          1.1337e+00, -2.0877e-01, -9.7329e-01, -1.8363e+00,  8.0424e-01,\n",
            "         -1.7196e+00,  2.0346e-01,  1.3009e+00,  4.6415e-01, -8.9180e-01,\n",
            "         -1.8896e+00,  1.0088e+00,  8.9262e-01,  2.2112e+00,  1.5731e-01,\n",
            "          7.1490e-01,  1.9452e-01,  5.6214e-02, -1.1290e+00, -1.5752e+00,\n",
            "         -3.8754e-01,  6.8447e-01,  5.0278e-01, -6.9443e-01,  3.8280e-02,\n",
            "         -2.5583e-01, -2.5136e-02,  1.2283e+00,  7.7656e-01,  9.1943e-01,\n",
            "          1.4152e+00, -1.2351e+00,  8.9282e-01,  1.8853e+00,  1.2512e-01,\n",
            "          6.2750e-01,  1.8391e+00, -5.2879e-01,  6.4196e-01, -7.4661e-01,\n",
            "         -6.9332e-01,  2.4011e+00,  2.7346e-02,  4.7083e-01, -7.2789e-01,\n",
            "         -3.4903e-01,  1.1220e+00,  2.2001e-01,  7.7737e-03, -1.6467e+00,\n",
            "          1.9680e-01,  6.0655e-01,  1.7157e-02, -9.3305e-02,  3.5859e-01,\n",
            "         -3.3113e-03,  9.6267e-01,  2.6110e+00,  6.8646e-01,  1.4828e-03,\n",
            "          3.6276e-01, -7.0912e-01, -2.3556e+00, -7.2689e-01,  1.0799e-01,\n",
            "          2.3207e+00, -5.9425e-01, -8.0462e-01,  1.1903e+00, -1.5101e-01,\n",
            "         -4.7016e-01, -1.8712e+00, -2.4973e-01,  8.2522e-01, -1.1223e+00,\n",
            "          8.2031e-01,  6.7115e-01, -3.8919e-01, -1.5631e+00, -7.2021e-02,\n",
            "          9.5390e-01,  3.5545e-01,  9.2818e-01,  4.6911e-01, -1.1909e+00,\n",
            "          4.5322e-01,  1.6245e-01,  4.6822e-01,  8.9126e-01,  1.8744e+00,\n",
            "          1.3984e-01, -2.1865e-01, -6.8005e-01, -7.4288e-01,  9.4058e-01,\n",
            "         -1.5988e+00,  1.2811e+00, -7.8907e-01,  1.6911e+00,  9.9028e-01,\n",
            "          1.1185e+00, -2.7788e-01,  9.5543e-01, -5.0556e-01, -2.1603e+00,\n",
            "         -1.2165e-01, -1.3796e+00,  5.5024e-01, -2.2459e-02,  2.3303e-01,\n",
            "          2.2832e-01,  6.7467e-03,  7.7231e-01,  4.2846e-03,  3.4620e-01,\n",
            "          3.9393e-01,  4.7336e-01, -4.1727e-01,  8.5216e-01,  2.0950e-01,\n",
            "         -1.0080e+00,  1.8271e-01,  8.0096e-01, -4.5537e-01, -7.5170e-01,\n",
            "          4.4042e-01, -1.3114e-01,  8.9067e-01, -1.0677e-01,  2.2415e+00,\n",
            "         -1.4101e+00, -9.0886e-01, -7.9302e-01, -3.5247e-01,  5.3886e-01,\n",
            "          1.3160e+00, -6.8935e-02,  7.7891e-01,  2.1373e-02, -6.6458e-01,\n",
            "          1.4832e+00,  3.3633e-01, -1.5087e+00,  4.6308e-01,  1.3206e+00,\n",
            "         -3.6630e-01, -1.5998e-01,  2.8343e-01,  1.7599e+00,  3.5121e-01,\n",
            "         -1.1614e+00,  2.4830e+00,  2.4324e-01,  1.1386e+00,  9.5402e-01,\n",
            "         -4.6546e-01, -6.9102e-01, -1.8885e-02,  9.5421e-01, -8.4550e-03,\n",
            "          1.0073e-01, -4.7235e-02,  1.9576e-01, -2.0663e-01, -1.5672e+00,\n",
            "          1.8775e-01, -1.0667e+00,  1.7751e+00, -1.0125e+00,  4.7895e-01,\n",
            "         -1.4526e+00, -8.1817e-01,  6.7103e-01,  1.0334e+00, -5.7445e-01,\n",
            "         -1.5068e+00, -1.7942e+00, -2.4605e+00,  1.3047e+00,  7.0822e-01,\n",
            "          1.7820e+00, -1.1579e+00,  9.6956e-01,  1.3970e+00,  8.8602e-01,\n",
            "         -5.6285e-01,  4.4243e-01,  8.7033e-01,  8.0565e-01,  2.2643e-01,\n",
            "         -1.1304e+00,  8.6093e-01, -3.1113e-01,  8.8712e-01, -4.9797e-01,\n",
            "          1.9540e+00,  1.0573e+00,  3.4215e-01, -1.9290e+00,  1.5963e+00,\n",
            "          8.6718e-01,  8.6931e-01, -4.7695e-01, -2.8006e-01, -1.2497e+00,\n",
            "          9.4932e-01,  2.6263e-02, -1.6256e+00,  9.6059e-01, -5.5998e-01,\n",
            "          2.2288e-01,  9.8320e-01,  2.8443e-01,  1.8558e+00,  2.6383e+00,\n",
            "          1.2488e+00, -5.7580e-01,  4.1469e-01,  1.2508e+00, -1.3232e+00,\n",
            "         -1.1588e-01,  1.7519e+00, -1.0899e+00,  3.4316e-01,  1.4987e+00,\n",
            "         -9.8150e-01,  1.0027e+00,  1.6948e+00,  3.6158e-01, -2.0132e+00,\n",
            "         -3.6196e-01,  1.9650e-01, -4.3778e-01, -3.9107e-02, -1.4330e-01,\n",
            "         -1.0730e+00,  4.1350e-01, -2.5011e-01,  1.0239e-01,  1.7290e+00,\n",
            "         -1.1522e+00,  2.1704e-01,  1.5222e+00,  2.1593e+00, -1.2997e-01,\n",
            "          1.0871e+00,  1.3845e+00, -1.0878e+00,  1.6103e+00, -1.5905e+00,\n",
            "          5.7137e-01, -1.8938e+00,  1.2965e+00, -1.0498e+00,  6.6904e-01,\n",
            "         -8.8675e-01, -1.9927e+00,  2.2317e+00, -9.2451e-01,  1.5939e+00,\n",
            "         -8.7393e-01,  3.0570e-01,  5.5012e-02, -5.7749e-01,  1.6918e+00,\n",
            "         -2.7591e-01, -7.1415e-01, -6.8740e-01,  4.6418e-01,  6.4874e-01,\n",
            "         -1.1506e+00, -6.2630e-01,  1.5423e+00,  1.5662e-01, -1.1310e-01,\n",
            "          1.8369e+00,  9.2649e-01,  6.0066e-01,  1.6356e+00,  7.4164e-01,\n",
            "         -7.5941e-01,  1.7149e+00, -9.0309e-01,  9.5444e-01,  6.3491e-01,\n",
            "         -2.9188e+00,  5.0774e-01,  2.3106e-01,  7.3993e-01, -1.0757e+00,\n",
            "          5.9230e-01,  1.0650e+00,  1.5066e-01, -1.8407e+00,  1.3804e+00,\n",
            "          1.8099e+00,  9.2607e-01,  5.5700e-01, -1.2212e+00, -2.4540e-01,\n",
            "         -1.1889e+00, -1.2615e+00, -3.2769e-01,  1.8969e+00, -3.9573e-01,\n",
            "          7.6886e-01,  1.4737e+00,  8.5087e-01,  1.3206e+00,  6.7492e-01,\n",
            "          7.3682e-01, -1.4192e-01, -1.8183e+00,  3.3392e-01,  1.3611e-03,\n",
            "          7.0200e-01, -3.1268e-01,  1.1951e+00, -4.7830e-01, -6.6962e-01,\n",
            "          1.3985e+00, -8.6218e-02,  1.5663e-02,  1.0044e+00,  5.2000e-01,\n",
            "         -7.4575e-01,  2.5688e+00,  8.7633e-02,  1.5797e+00, -1.1691e+00,\n",
            "          4.9038e-01,  4.6673e-01,  1.3116e+00,  2.8522e-01,  1.0150e-01,\n",
            "          3.2659e-01,  6.7305e-01,  1.9768e-01, -4.8511e-02,  1.8242e+00,\n",
            "          1.5330e+00,  4.4466e-01,  9.6605e-01,  2.0891e-01,  8.7401e-01,\n",
            "         -1.7572e-01, -4.4288e-01,  9.8908e-01,  2.2995e+00,  3.2932e-01,\n",
            "         -3.3575e-01,  5.4761e-01, -4.1516e-01,  4.3998e-03, -1.0186e-02,\n",
            "          1.7081e+00,  7.4811e-01, -1.1678e+00,  6.8664e-01,  7.2794e-01,\n",
            "          7.1041e-02, -1.3011e+00,  7.8389e-01, -8.7812e-01,  1.3461e+00,\n",
            "         -1.0163e+00,  1.7212e+00,  3.9043e-01,  9.6227e-01, -8.3254e-01,\n",
            "          4.5086e-01,  5.1856e-01,  9.8015e-01,  3.4401e-01, -1.1470e+00,\n",
            "          2.5996e-01, -4.6494e-01,  1.5802e+00,  1.6867e+00,  1.6425e+00,\n",
            "         -1.2626e+00,  1.7484e-01,  1.3488e-01, -1.7402e+00,  1.2969e+00,\n",
            "         -3.7401e-01,  4.0341e-01,  7.8936e-01,  1.1552e+00,  2.4409e+00,\n",
            "          5.4018e-01,  6.4766e-01,  1.4521e-01,  3.8361e-01,  1.6536e+00,\n",
            "         -1.8584e+00,  4.7237e-01, -1.8005e-01, -3.3551e-01,  2.4040e+00,\n",
            "         -9.0665e-01, -5.7709e-01,  5.8381e-01,  1.5959e-01, -3.5200e-01,\n",
            "         -2.9257e-01,  1.5517e+00,  1.6036e+00,  1.1049e+00, -3.0659e-01,\n",
            "          8.7755e-01,  1.7404e-01, -5.5050e-01,  2.1968e+00, -5.4800e-02,\n",
            "         -1.0245e+00, -9.0937e-01, -4.2242e-01, -3.5559e-01,  2.3411e-01,\n",
            "         -1.8961e+00, -3.2426e-02,  1.0025e+00,  6.2592e-01, -1.3327e+00,\n",
            "          2.1677e-01,  4.5579e-01, -8.3488e-01,  7.1169e-01,  6.1742e-01,\n",
            "         -1.2807e+00, -7.5230e-01, -2.8575e-01,  4.2627e-01, -1.1519e-01,\n",
            "          1.7113e+00, -3.6324e-01,  6.2965e-01, -1.1591e+00,  2.1348e+00,\n",
            "          1.4781e+00,  2.6737e+00, -1.1703e+00,  2.3814e-01,  1.5739e-01,\n",
            "         -7.2363e-01,  1.9570e+00,  1.3477e+00, -1.6697e+00,  6.8577e-01,\n",
            "          1.9864e+00, -1.6415e+00, -1.7791e-01, -1.3468e+00,  9.1334e-01,\n",
            "         -7.8481e-01,  1.6235e+00,  1.0686e+00, -9.7003e-01, -1.3965e+00,\n",
            "         -1.6919e+00, -9.0862e-01, -4.6273e-01,  1.0658e+00,  1.2017e-01,\n",
            "         -3.2849e-01,  4.4676e-02,  1.3644e+00, -8.7457e-01, -1.5334e+00,\n",
            "          1.7887e-01,  1.4287e+00, -3.7701e-01, -1.3136e+00, -1.4604e-01,\n",
            "          5.1751e-01, -1.0399e-01,  1.2479e+00,  2.3397e+00,  3.6733e-01,\n",
            "          1.0917e+00,  1.1190e-02, -4.7464e-02, -1.2932e+00,  3.1999e-01,\n",
            "         -7.7405e-01, -4.0441e-01,  2.2218e+00,  4.5132e-01,  2.0220e+00,\n",
            "         -1.0406e-01,  1.7415e+00,  1.6375e+00,  1.2847e+00,  1.7764e+00,\n",
            "          6.0493e-01,  1.8856e+00,  1.2546e+00,  7.3737e-01,  1.5125e+00,\n",
            "          1.3100e+00,  5.1960e-01,  6.8177e-01,  5.6165e-01,  1.0763e-01,\n",
            "          3.2425e-01,  8.3909e-01, -6.1835e-01, -5.0002e-02,  4.3011e-01,\n",
            "         -1.2309e+00, -5.7340e-01, -9.5635e-01, -9.9611e-01,  1.9160e-01,\n",
            "          4.4012e-01,  2.0608e+00, -2.2777e-01, -6.0421e-01, -1.9652e+00,\n",
            "          2.5480e-01, -1.0116e+00, -1.1112e+00, -1.6990e+00,  1.1059e-01,\n",
            "         -1.1693e+00, -1.6786e+00, -1.4337e+00, -2.0690e+00, -1.4222e+00,\n",
            "         -1.1874e+00, -1.2686e+00, -1.3890e+00, -1.7546e+00, -8.3735e-01,\n",
            "         -1.9694e-01, -4.7911e-01, -2.5977e-01, -4.0726e-01, -1.9306e+00,\n",
            "         -3.7464e-01, -1.2813e+00, -1.3014e+00,  3.3303e-01, -4.5705e-01,\n",
            "         -6.7607e-01, -2.5896e-01, -1.0624e+00, -8.0823e-01, -9.9576e-01,\n",
            "         -2.0364e+00, -1.9078e+00, -1.1812e+00, -2.4760e-01, -1.2174e+00,\n",
            "         -1.0250e+00, -9.6908e-01, -1.5964e+00, -1.8124e+00, -1.4917e+00,\n",
            "         -1.4575e+00,  6.8099e-01, -1.9782e-01,  7.2319e-01,  2.1947e-01,\n",
            "         -6.3504e-01,  9.0749e-01, -9.3020e-01, -1.1633e+00,  1.1875e+00,\n",
            "          1.0996e-01, -3.0598e-01,  5.9397e-01,  3.5554e-01, -1.0320e+00,\n",
            "          4.5852e-01, -1.7264e-02, -9.9292e-01, -5.6485e-01, -8.3827e-01,\n",
            "         -1.5039e+00, -1.2357e+00, -6.4257e-01, -7.2143e-01, -1.2031e-01,\n",
            "         -1.8690e+00, -1.3193e+00, -1.1603e+00, -1.9892e+00, -7.8842e-01,\n",
            "         -9.6700e-01, -1.5960e+00, -1.9164e+00, -1.6558e-02,  1.5575e+00]],\n",
            "       device='cuda:0')\n",
            "time : 83.0018801689148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2739bcb1",
        "outputId": "0e47d631-2101-4461-a95a-6460ba6b27e3"
      },
      "source": [
        "network2.change_status()\n",
        "eval(network2, test_tensor)"
      ],
      "id": "2739bcb1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "tensor([[-7.4385e-01,  1.4804e+00, -5.3962e-01,  5.4345e-01, -4.1696e-01,\n",
            "         -1.8613e-01, -6.0308e-01, -4.6140e-01, -5.9823e-01,  8.8838e-03,\n",
            "          4.5382e-01,  1.2223e+00,  9.3680e-01,  1.0030e+00,  7.3924e-01,\n",
            "          5.4289e-01,  2.3086e-01, -2.5446e-01,  1.1991e-01,  8.6787e-01,\n",
            "          2.4375e-01,  6.2169e-01,  4.5873e-02, -3.5269e-01,  4.0353e-01,\n",
            "         -6.8085e-01,  7.5815e-01, -1.4653e-03, -1.1122e+00,  2.7171e-01,\n",
            "         -1.1803e+00, -5.9074e-02, -4.6356e-01, -4.3567e-01, -3.3285e-01,\n",
            "         -8.0615e-01,  2.6849e-01, -1.7941e+00,  4.1561e-01, -1.3428e+00,\n",
            "         -1.1537e-02, -3.6764e-01, -1.6046e-01,  1.2216e-01, -6.0874e-01,\n",
            "         -7.0940e-01, -2.6716e-01, -3.0937e-01, -1.1674e+00, -2.4726e-01,\n",
            "          5.9597e-01, -1.2327e+00,  2.5826e-01,  5.8163e-01, -1.1394e-01,\n",
            "         -4.9212e-01, -4.9626e-01, -1.1175e+00,  1.0810e+00,  4.5191e-01,\n",
            "         -1.1261e-01, -1.3682e+00, -7.1520e-01,  8.7955e-01,  7.0190e-01,\n",
            "          4.8610e-01,  2.2557e-02, -4.0715e-01, -4.1010e-01,  1.0453e-01,\n",
            "          2.6733e-01,  2.4213e-01,  1.0820e+00,  2.1129e+00,  5.0970e-01,\n",
            "          1.8633e+00, -8.3764e-02,  1.3136e+00,  1.6341e+00,  1.7486e+00,\n",
            "          1.5863e+00,  3.2545e-01, -9.1828e-02,  1.3887e+00, -6.4163e-01,\n",
            "          8.5933e-01,  5.4783e-01,  1.1240e+00, -1.3046e-01, -8.5279e-01,\n",
            "         -7.4634e-01,  2.7560e-01,  9.3578e-01, -5.3383e-01,  4.6995e-01,\n",
            "         -9.9149e-02, -2.8899e-01, -7.5107e-01,  4.3313e-01, -2.7696e-01,\n",
            "          3.5376e-01, -1.1748e+00, -1.3980e+00,  1.8103e+00, -7.5392e-01,\n",
            "         -1.5799e+00, -6.6526e-01,  2.2055e-01, -1.0307e+00, -1.6654e+00,\n",
            "         -1.8539e-01,  3.4019e+00,  3.6206e-01, -1.3780e-01, -5.1835e-01,\n",
            "         -6.4013e-01, -1.3116e+00,  4.3284e-01, -1.3119e+00, -1.6515e+00,\n",
            "         -5.7537e-01, -1.7243e+00, -9.3877e-01, -1.2554e+00, -7.0863e-01,\n",
            "         -1.2459e+00,  9.4431e-01, -5.7970e-01,  7.3166e-01, -4.8994e-01,\n",
            "         -3.2050e-01,  7.6387e-02,  4.7053e-01,  3.4763e-01,  1.1433e-01,\n",
            "         -2.8045e-01, -2.2304e-01,  6.5024e-01,  9.7364e-01, -7.5421e-02,\n",
            "         -5.5115e-01,  1.1297e+00,  1.7079e-01, -2.6345e-01, -4.1528e-01,\n",
            "          3.7342e-01, -4.0163e-01,  1.9580e-01, -6.0910e-01, -1.3388e-02,\n",
            "         -9.9800e-04, -1.1082e-01, -3.2525e-01,  7.0204e-01, -5.1442e-02,\n",
            "          3.3909e-01, -6.7056e-02, -2.7561e-02, -2.2384e-01, -5.7282e-01,\n",
            "          2.0210e-01,  1.6263e-01, -9.6562e-02,  3.6492e-01,  2.2467e-03,\n",
            "         -1.5999e-01,  2.0206e-01, -4.2065e-01, -2.0763e-01, -1.0544e-02,\n",
            "         -3.4059e-01,  1.8549e-01, -2.1002e-01, -4.1591e-01, -4.3183e-01,\n",
            "         -1.7493e-01, -8.2035e-01, -1.8623e-02, -3.3839e-01, -8.0877e-01,\n",
            "         -7.8285e-01, -2.3067e-01, -9.1903e-02,  2.4656e-01,  2.3014e-01,\n",
            "         -3.7154e-01, -2.3701e-01,  4.2178e-01,  5.7082e-02, -3.3908e-01,\n",
            "          1.5777e-01,  4.3404e-01, -2.1312e-01,  2.1297e-01,  2.7286e-01,\n",
            "         -2.0891e-01,  6.4960e-02, -4.1091e-01, -3.4761e-01, -2.0758e-02,\n",
            "         -6.6599e-02, -4.5688e-01,  3.4935e-01,  7.4217e-01,  3.6380e-02,\n",
            "         -5.2469e-01, -5.6685e-01, -2.3823e-01, -6.5064e-01, -2.1959e-01,\n",
            "          2.5911e-01, -2.2888e-01,  4.0978e-01, -4.3934e-01, -2.6504e-01,\n",
            "          3.7995e-02, -2.3965e-01,  4.1477e-01, -3.8380e-01, -3.7248e-01,\n",
            "         -3.3986e-01,  5.1973e-02, -4.8214e-01, -4.1012e-02, -8.6318e-02,\n",
            "         -5.4240e-01, -7.1135e-02, -3.9521e-01,  1.0748e-01,  4.7852e-01,\n",
            "         -3.4945e-02, -4.6166e-01, -2.2893e-01, -2.1084e-01, -2.6981e-01,\n",
            "         -4.5201e-01,  3.3378e-02,  4.3637e-02, -4.5892e-01,  1.8745e-01,\n",
            "         -4.4406e-01,  6.0613e-02,  4.4952e-02, -6.4187e-01, -1.4272e+00,\n",
            "         -1.6164e-01, -6.0419e-01, -4.8529e-01, -5.0862e-01, -9.9468e-02,\n",
            "         -7.5788e-02, -3.0424e-01,  2.9389e-02, -3.3757e-01,  2.6532e-01,\n",
            "         -9.6890e-01, -3.6888e-01, -2.9820e-01,  3.3749e-01,  1.1886e-01,\n",
            "         -2.1230e-01, -1.5888e-01, -6.3335e-01, -1.1734e-01, -4.0291e-01,\n",
            "          2.4844e-01, -5.5545e-01, -6.5205e-01, -1.0902e+00,  3.6591e-02,\n",
            "          4.1445e-01, -1.8908e-01,  4.6877e-01,  3.7681e-01,  3.5081e-01,\n",
            "         -1.6425e-01, -9.2678e-02,  3.8070e-01, -2.2806e-02,  6.5359e-01,\n",
            "          4.9850e-01,  8.0728e-01,  1.1169e+00,  1.1384e+00,  6.8824e-01,\n",
            "          9.5939e-01, -3.4475e-01, -7.0813e-02, -7.1439e-01, -6.8659e-01,\n",
            "         -1.3462e+00, -8.3473e-01, -1.6688e-01,  1.3983e-01, -4.8327e-01,\n",
            "         -7.3752e-01,  4.8660e-01, -2.0548e-02,  5.7182e-01,  9.1997e-01,\n",
            "         -8.8592e-01,  6.0342e-01,  1.1056e-01, -2.8971e-01, -3.4949e-01,\n",
            "         -1.2913e+00, -7.7607e-01,  2.5860e-01,  4.2592e-01, -9.6248e-01,\n",
            "          1.0391e+00,  2.6247e-01,  1.9228e-01,  5.4780e-01,  8.0445e-01,\n",
            "         -9.2695e-02,  1.3298e-01, -1.2238e-02,  1.6392e+00,  1.0296e+00,\n",
            "          8.2907e-01, -1.4700e+00, -5.9922e-01, -1.2784e+00, -1.1546e+00,\n",
            "         -1.3563e+00, -3.4128e-01, -3.2267e-01, -1.3861e+00, -1.4859e+00,\n",
            "         -1.3301e-01,  6.7848e-01,  4.6709e-01, -5.6896e-01, -5.3764e-01,\n",
            "          9.8049e-02, -1.0103e+00,  9.8881e-01, -9.7044e-01, -1.5368e+00,\n",
            "         -4.6683e-01, -1.4186e+00, -1.3591e+00, -1.3577e+00, -6.4267e-01,\n",
            "         -1.2442e+00, -1.4280e+00, -8.0808e-01, -1.4790e+00, -8.6885e-01,\n",
            "         -7.4794e-01,  1.2914e-01, -1.4433e-01,  1.2823e-01, -2.3973e-01,\n",
            "         -7.3057e-01,  6.0422e-01,  2.7698e-01,  1.4321e-01,  7.5630e-01,\n",
            "          3.5549e-01,  1.6275e-01,  1.6276e-01, -2.7816e-01, -8.6404e-01,\n",
            "         -5.0997e-01, -2.5736e-01, -3.2262e-01, -4.8008e-01, -5.7317e-01,\n",
            "         -7.6263e-02,  5.3617e-01, -6.1445e-01, -7.7106e-01,  4.4485e-01,\n",
            "         -9.7659e-01, -9.3122e-04,  4.3836e-01, -6.1316e-01, -1.0477e+00,\n",
            "          1.9348e-02, -9.9145e-01, -4.6660e-01,  2.5218e-01, -3.5016e-01,\n",
            "         -9.9596e-01, -1.2644e+00, -8.5284e-01, -9.7842e-01,  7.1816e-01,\n",
            "         -4.9781e-01,  1.3865e-02, -1.8117e-01, -1.3819e+00, -3.6781e-01,\n",
            "          1.4797e+00, -8.2545e-01, -2.9626e-01, -8.5989e-01,  1.2948e+00,\n",
            "         -3.0467e-01, -1.0858e+00,  7.3612e-01, -6.1273e-01,  7.1873e-01,\n",
            "          1.1337e+00, -2.0877e-01, -9.7329e-01, -1.8363e+00,  8.0424e-01,\n",
            "         -1.7196e+00,  2.0346e-01,  1.3009e+00,  4.6415e-01, -8.9180e-01,\n",
            "         -1.8896e+00,  1.0088e+00,  8.9262e-01,  2.2112e+00,  1.5731e-01,\n",
            "          7.1490e-01,  1.9452e-01,  5.6214e-02, -1.1290e+00, -1.5752e+00,\n",
            "         -3.8754e-01,  6.8447e-01,  5.0278e-01, -6.9443e-01,  3.8280e-02,\n",
            "         -2.5583e-01, -2.5136e-02,  1.2283e+00,  7.7656e-01,  9.1943e-01,\n",
            "          1.4152e+00, -1.2351e+00,  8.9282e-01,  1.8853e+00,  1.2512e-01,\n",
            "          6.2750e-01,  1.8391e+00, -5.2879e-01,  6.4196e-01, -7.4661e-01,\n",
            "         -6.9332e-01,  2.4011e+00,  2.7346e-02,  4.7083e-01, -7.2789e-01,\n",
            "         -3.4903e-01,  1.1220e+00,  2.2001e-01,  7.7737e-03, -1.6467e+00,\n",
            "          1.9680e-01,  6.0655e-01,  1.7157e-02, -9.3305e-02,  3.5859e-01,\n",
            "         -3.3113e-03,  9.6267e-01,  2.6110e+00,  6.8646e-01,  1.4828e-03,\n",
            "          3.6276e-01, -7.0912e-01, -2.3556e+00, -7.2689e-01,  1.0799e-01,\n",
            "          2.3207e+00, -5.9425e-01, -8.0462e-01,  1.1903e+00, -1.5101e-01,\n",
            "         -4.7016e-01, -1.8712e+00, -2.4973e-01,  8.2522e-01, -1.1223e+00,\n",
            "          8.2031e-01,  6.7115e-01, -3.8919e-01, -1.5631e+00, -7.2021e-02,\n",
            "          9.5390e-01,  3.5545e-01,  9.2818e-01,  4.6911e-01, -1.1909e+00,\n",
            "          4.5322e-01,  1.6245e-01,  4.6822e-01,  8.9126e-01,  1.8744e+00,\n",
            "          1.3984e-01, -2.1865e-01, -6.8005e-01, -7.4288e-01,  9.4058e-01,\n",
            "         -1.5988e+00,  1.2811e+00, -7.8907e-01,  1.6911e+00,  9.9028e-01,\n",
            "          1.1185e+00, -2.7788e-01,  9.5543e-01, -5.0556e-01, -2.1603e+00,\n",
            "         -1.2165e-01, -1.3796e+00,  5.5024e-01, -2.2459e-02,  2.3303e-01,\n",
            "          2.2832e-01,  6.7467e-03,  7.7231e-01,  4.2846e-03,  3.4620e-01,\n",
            "          3.9393e-01,  4.7336e-01, -4.1727e-01,  8.5216e-01,  2.0950e-01,\n",
            "         -1.0080e+00,  1.8271e-01,  8.0096e-01, -4.5537e-01, -7.5170e-01,\n",
            "          4.4042e-01, -1.3114e-01,  8.9067e-01, -1.0677e-01,  2.2415e+00,\n",
            "         -1.4101e+00, -9.0886e-01, -7.9302e-01, -3.5247e-01,  5.3886e-01,\n",
            "          1.3160e+00, -6.8935e-02,  7.7891e-01,  2.1373e-02, -6.6458e-01,\n",
            "          1.4832e+00,  3.3633e-01, -1.5087e+00,  4.6308e-01,  1.3206e+00,\n",
            "         -3.6630e-01, -1.5998e-01,  2.8343e-01,  1.7599e+00,  3.5121e-01,\n",
            "         -1.1614e+00,  2.4830e+00,  2.4324e-01,  1.1386e+00,  9.5402e-01,\n",
            "         -4.6546e-01, -6.9102e-01, -1.8885e-02,  9.5421e-01, -8.4550e-03,\n",
            "          1.0073e-01, -4.7235e-02,  1.9576e-01, -2.0663e-01, -1.5672e+00,\n",
            "          1.8775e-01, -1.0667e+00,  1.7751e+00, -1.0125e+00,  4.7895e-01,\n",
            "         -1.4526e+00, -8.1817e-01,  6.7103e-01,  1.0334e+00, -5.7445e-01,\n",
            "         -1.5068e+00, -1.7942e+00, -2.4605e+00,  1.3047e+00,  7.0822e-01,\n",
            "          1.7820e+00, -1.1579e+00,  9.6956e-01,  1.3970e+00,  8.8602e-01,\n",
            "         -5.6285e-01,  4.4243e-01,  8.7033e-01,  8.0565e-01,  2.2643e-01,\n",
            "         -1.1304e+00,  8.6093e-01, -3.1113e-01,  8.8712e-01, -4.9797e-01,\n",
            "          1.9540e+00,  1.0573e+00,  3.4215e-01, -1.9290e+00,  1.5963e+00,\n",
            "          8.6718e-01,  8.6931e-01, -4.7695e-01, -2.8006e-01, -1.2497e+00,\n",
            "          9.4932e-01,  2.6263e-02, -1.6256e+00,  9.6059e-01, -5.5998e-01,\n",
            "          2.2288e-01,  9.8320e-01,  2.8443e-01,  1.8558e+00,  2.6383e+00,\n",
            "          1.2488e+00, -5.7580e-01,  4.1469e-01,  1.2508e+00, -1.3232e+00,\n",
            "         -1.1588e-01,  1.7519e+00, -1.0899e+00,  3.4316e-01,  1.4987e+00,\n",
            "         -9.8150e-01,  1.0027e+00,  1.6948e+00,  3.6158e-01, -2.0132e+00,\n",
            "         -3.6196e-01,  1.9650e-01, -4.3778e-01, -3.9107e-02, -1.4330e-01,\n",
            "         -1.0730e+00,  4.1350e-01, -2.5011e-01,  1.0239e-01,  1.7290e+00,\n",
            "         -1.1522e+00,  2.1704e-01,  1.5222e+00,  2.1593e+00, -1.2997e-01,\n",
            "          1.0871e+00,  1.3845e+00, -1.0878e+00,  1.6103e+00, -1.5905e+00,\n",
            "          5.7137e-01, -1.8938e+00,  1.2965e+00, -1.0498e+00,  6.6904e-01,\n",
            "         -8.8675e-01, -1.9927e+00,  2.2317e+00, -9.2451e-01,  1.5939e+00,\n",
            "         -8.7393e-01,  3.0570e-01,  5.5012e-02, -5.7749e-01,  1.6918e+00,\n",
            "         -2.7591e-01, -7.1415e-01, -6.8740e-01,  4.6418e-01,  6.4874e-01,\n",
            "         -1.1506e+00, -6.2630e-01,  1.5423e+00,  1.5662e-01, -1.1310e-01,\n",
            "          1.8369e+00,  9.2649e-01,  6.0066e-01,  1.6356e+00,  7.4164e-01,\n",
            "         -7.5941e-01,  1.7149e+00, -9.0309e-01,  9.5444e-01,  6.3491e-01,\n",
            "         -2.9188e+00,  5.0774e-01,  2.3106e-01,  7.3993e-01, -1.0757e+00,\n",
            "          5.9230e-01,  1.0650e+00,  1.5066e-01, -1.8407e+00,  1.3804e+00,\n",
            "          1.8099e+00,  9.2607e-01,  5.5700e-01, -1.2212e+00, -2.4540e-01,\n",
            "         -1.1889e+00, -1.2615e+00, -3.2769e-01,  1.8969e+00, -3.9573e-01,\n",
            "          7.6886e-01,  1.4737e+00,  8.5087e-01,  1.3206e+00,  6.7492e-01,\n",
            "          7.3682e-01, -1.4192e-01, -1.8183e+00,  3.3392e-01,  1.3611e-03,\n",
            "          7.0200e-01, -3.1268e-01,  1.1951e+00, -4.7830e-01, -6.6962e-01,\n",
            "          1.3985e+00, -8.6218e-02,  1.5663e-02,  1.0044e+00,  5.2000e-01,\n",
            "         -7.4575e-01,  2.5688e+00,  8.7633e-02,  1.5797e+00, -1.1691e+00,\n",
            "          4.9038e-01,  4.6673e-01,  1.3116e+00,  2.8522e-01,  1.0150e-01,\n",
            "          3.2659e-01,  6.7305e-01,  1.9768e-01, -4.8511e-02,  1.8242e+00,\n",
            "          1.5330e+00,  4.4466e-01,  9.6605e-01,  2.0891e-01,  8.7401e-01,\n",
            "         -1.7572e-01, -4.4288e-01,  9.8908e-01,  2.2995e+00,  3.2932e-01,\n",
            "         -3.3575e-01,  5.4761e-01, -4.1516e-01,  4.3998e-03, -1.0186e-02,\n",
            "          1.7081e+00,  7.4811e-01, -1.1678e+00,  6.8664e-01,  7.2794e-01,\n",
            "          7.1041e-02, -1.3011e+00,  7.8389e-01, -8.7812e-01,  1.3461e+00,\n",
            "         -1.0163e+00,  1.7212e+00,  3.9043e-01,  9.6227e-01, -8.3254e-01,\n",
            "          4.5086e-01,  5.1856e-01,  9.8015e-01,  3.4401e-01, -1.1470e+00,\n",
            "          2.5996e-01, -4.6494e-01,  1.5802e+00,  1.6867e+00,  1.6425e+00,\n",
            "         -1.2626e+00,  1.7484e-01,  1.3488e-01, -1.7402e+00,  1.2969e+00,\n",
            "         -3.7401e-01,  4.0341e-01,  7.8936e-01,  1.1552e+00,  2.4409e+00,\n",
            "          5.4018e-01,  6.4766e-01,  1.4521e-01,  3.8361e-01,  1.6536e+00,\n",
            "         -1.8584e+00,  4.7237e-01, -1.8005e-01, -3.3551e-01,  2.4040e+00,\n",
            "         -9.0665e-01, -5.7709e-01,  5.8381e-01,  1.5959e-01, -3.5200e-01,\n",
            "         -2.9257e-01,  1.5517e+00,  1.6036e+00,  1.1049e+00, -3.0659e-01,\n",
            "          8.7755e-01,  1.7404e-01, -5.5050e-01,  2.1968e+00, -5.4800e-02,\n",
            "         -1.0245e+00, -9.0937e-01, -4.2242e-01, -3.5559e-01,  2.3411e-01,\n",
            "         -1.8961e+00, -3.2426e-02,  1.0025e+00,  6.2592e-01, -1.3327e+00,\n",
            "          2.1677e-01,  4.5579e-01, -8.3488e-01,  7.1169e-01,  6.1742e-01,\n",
            "         -1.2807e+00, -7.5230e-01, -2.8575e-01,  4.2627e-01, -1.1519e-01,\n",
            "          1.7113e+00, -3.6324e-01,  6.2965e-01, -1.1591e+00,  2.1348e+00,\n",
            "          1.4781e+00,  2.6737e+00, -1.1703e+00,  2.3814e-01,  1.5739e-01,\n",
            "         -7.2363e-01,  1.9570e+00,  1.3477e+00, -1.6697e+00,  6.8577e-01,\n",
            "          1.9864e+00, -1.6415e+00, -1.7791e-01, -1.3468e+00,  9.1334e-01,\n",
            "         -7.8481e-01,  1.6235e+00,  1.0686e+00, -9.7003e-01, -1.3965e+00,\n",
            "         -1.6919e+00, -9.0862e-01, -4.6273e-01,  1.0658e+00,  1.2017e-01,\n",
            "         -3.2849e-01,  4.4676e-02,  1.3644e+00, -8.7457e-01, -1.5334e+00,\n",
            "          1.7887e-01,  1.4287e+00, -3.7701e-01, -1.3136e+00, -1.4604e-01,\n",
            "          5.1751e-01, -1.0399e-01,  1.2479e+00,  2.3397e+00,  3.6733e-01,\n",
            "          1.0917e+00,  1.1190e-02, -4.7464e-02, -1.2932e+00,  3.1999e-01,\n",
            "         -7.7405e-01, -4.0441e-01,  2.2218e+00,  4.5132e-01,  2.0220e+00,\n",
            "         -1.0406e-01,  1.7415e+00,  1.6375e+00,  1.2847e+00,  1.7764e+00,\n",
            "          6.0493e-01,  1.8856e+00,  1.2546e+00,  7.3737e-01,  1.5125e+00,\n",
            "          1.3100e+00,  5.1960e-01,  6.8177e-01,  5.6165e-01,  1.0763e-01,\n",
            "          3.2425e-01,  8.3909e-01, -6.1835e-01, -5.0002e-02,  4.3011e-01,\n",
            "         -1.2309e+00, -5.7340e-01, -9.5635e-01, -9.9611e-01,  1.9160e-01,\n",
            "          4.4012e-01,  2.0608e+00, -2.2777e-01, -6.0421e-01, -1.9652e+00,\n",
            "          2.5480e-01, -1.0116e+00, -1.1112e+00, -1.6990e+00,  1.1059e-01,\n",
            "         -1.1693e+00, -1.6786e+00, -1.4337e+00, -2.0690e+00, -1.4222e+00,\n",
            "         -1.1874e+00, -1.2686e+00, -1.3890e+00, -1.7546e+00, -8.3735e-01,\n",
            "         -1.9694e-01, -4.7911e-01, -2.5977e-01, -4.0726e-01, -1.9306e+00,\n",
            "         -3.7464e-01, -1.2813e+00, -1.3014e+00,  3.3303e-01, -4.5705e-01,\n",
            "         -6.7607e-01, -2.5896e-01, -1.0624e+00, -8.0823e-01, -9.9576e-01,\n",
            "         -2.0364e+00, -1.9078e+00, -1.1812e+00, -2.4760e-01, -1.2174e+00,\n",
            "         -1.0250e+00, -9.6908e-01, -1.5964e+00, -1.8124e+00, -1.4917e+00,\n",
            "         -1.4575e+00,  6.8099e-01, -1.9782e-01,  7.2319e-01,  2.1947e-01,\n",
            "         -6.3504e-01,  9.0749e-01, -9.3020e-01, -1.1633e+00,  1.1875e+00,\n",
            "          1.0996e-01, -3.0598e-01,  5.9397e-01,  3.5554e-01, -1.0320e+00,\n",
            "          4.5852e-01, -1.7264e-02, -9.9292e-01, -5.6485e-01, -8.3827e-01,\n",
            "         -1.5039e+00, -1.2357e+00, -6.4257e-01, -7.2143e-01, -1.2031e-01,\n",
            "         -1.8690e+00, -1.3193e+00, -1.1603e+00, -1.9892e+00, -7.8842e-01,\n",
            "         -9.6700e-01, -1.5960e+00, -1.9164e+00, -1.6558e-02,  1.5575e+00]],\n",
            "       device='cuda:0')\n",
            "time : 142.58330869674683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e975fa4"
      },
      "source": [
        ""
      ],
      "id": "5e975fa4",
      "execution_count": null,
      "outputs": []
    }
  ]
}